---
layout: default
title: Test Oracle Genearation
type: Homework
number: 02
active_tab: homework
release_date: 2025-02-19
due_date: 2025-03-05 23:59:00EDT

---

<!-- Check whether the assignment is ready to release -->
{% capture today %}{{'now' | date: '%s'}}{% endcapture %}
{% capture release_date %}{{page.release_date | date: '%s'}}{% endcapture %}
{% if release_date > today %} 
<div class="alert alert-danger">
Warning: this assignment is out of date.  It may still need to be updated for this year's class.  Check with your instructor before you start working on this assignment.
</div>
{% endif %}
<!-- End of check whether the assignment is up to date -->


<!-- Check whether the assignment is up to date -->
{% capture this_year %}{{'now' | date: '%Y'}}{% endcapture %}
{% capture due_year %}{{page.due_date | date: '%Y'}}{% endcapture %}
{% if this_year != due_year %} 
<div class="alert alert-danger">
Warning: this assignment is out of date.  It may still need to be updated for this year's class.  Check with your instructor before you start working on this assignment.
</div>
{% endif %}
<!-- End of check whether the assignment is up to date -->



{% if page.materials %}
<div class="alert alert-info">
You can download the materials for this assignment here:
<ul>
{% for item in page.materials %}
<li><a href="{{item.url}}">{{ item.name }}</a></li>
{% endfor %}
</ul>


<i>Remember to make a copy of the notebook into your own Drive by choosing “Save a Copy in Drive” from Colab’s “File” menu.</i>

</div>
{% endif %}





{{page.type}} {{page.number}}: {{page.title}}
=============================================================

_Due: {{page.due_date}}_

You can optionally work with a partner on this assignment. You clearly state in your sumbission if you are working with a partner and their name.

Given an input for a system, the challenge of distinguishing the corresponding desired, correct behaviour from potentially incorrect behavior is called the *test oracle problem*. 

## Overview

In this assignment, you will implement and evaluate two algorithms for test oracle generation:

- Neural Approach 
- IR Approach

### Setup: 

1. Get a free API key from [Hugging Face](https://huggingface.co/)  
      a. Create an account  
      b. Create a new [access token](https://huggingface.co/settings/tokens). The only permission needed is **"Make calls to inference providers"**. Save the token somewhere safe.  

2. Agree to Mistral's [terms of use](https://huggingface.co/mistralai/Mistral-7B-v0.1)

## 1. Neural Approach

Given a test prefix and a method under test, use a language model to generate the assertion.

**Program Representation** 
Language model based approaches consider the program as a sequence of words. 

### A Background on Large Language Models

The core task for most state-of-the-art LLMs is word prediction. Given a sequence of words, what is the probability distribution of the next word?

For example, given the sequence "Listen to your -" the most likely next words might be: heart, gut, body, parents, grandma, etc. This might look like the probability distribution shown below.

[Next Word Probability](https://towardsdatascience.com/wp-content/uploads/2023/07/1_yFX4vrio7Io2tnVWbW8xQ.png)

**Training**
For this course, we won't go into the specifics of training an LLM. You can think of the training stage as creating a *black-box* which calculates this probability distribution by looking at a large corpus of text.  After training, we have an artifact which, given a sequence of text, can output the next most probable token.

The training portion is highly expensive, costing millions of dollars and taking months to complete. Even with state of the art hardware! Luckily, LLMs like ChatGPT, Gemini, and DeepSeek are "pretrained" andwe can query the model without needing to do any training ourselves.

**Inference**
Inference refers to *querying* the model. This is much less expensive than training and does not modify the underlying model. This is what happens when you ask chatGPT a question in the web interface.

### Querying an LLM to Generate Assertions

I've included [starter code](https://github.com/bmc-cs-software-analysis/383/tree/main/hws/hw02/queryModel.sh) to query an LLM. For this assignment we'll use Mistral-7b: a model trained on both code and natural language. The "7b" refers to the size of the model (7 billion parameters). This is quite small in terms of modern LLMs. Chat-GPT4 has approximately 1.8 *trillion* parameters. 

To begin, open the starter code and save your `API_KEY` in the variable on line 1. Run `bash queryModel.sh` to see the response to the query (prompt) on line 2: `"Where is the best airport in the United States?"`

The output is returned as a json string with the output text in `choices[0].message.content`.

Try it out on a few other input prompts of your choice.

**Task: Generating an Assertion**

Now let's use this model to generate assertions given a test prefix. 

Download the prefixes and assertions using:
`wget https://bmc-cs-software-analysis.github.io/383/hws/hw02/testPrefixes.txt` 
and  
`wget https://bmc-cs-software-analysis.github.io/383/hws/hw02/assertions.txt`

Open the `testPrefixes` file and notice that the assertions are replaced with placeholders. The developer written assertions for each row are given in `assertions.txt`.

For each test prefix, ask the LLM to generate the assertion by placing it in the prompt and querying the model like you would chatGPT.

If you notice your results are poor, update your prompt. LLMs are highly sensitive to input prompts and small changes in your wording can lead to large changes in output performance. Tinker around with different ways to query the LLM until you find one you are satisifed with.

Record the output on each test prefix.

**Task: Ranking Assertions**

In this part of the assignment you will limit the output of the LLM according to a given formal grammar. You will record the outputs and observe the diferences from the previous step.

Given the following grammar, update your prompt to rank a list of all possible assertions for each prefix. 

** Formal Grammar **

** Non-terminals: **
- `A`: Assertion statement
- `const`: Constant value (0, 1, 5)
- `var`: Variable name that must match the expected type. (assertFalse expects a boolean, assertEquals expects numeric types, and assertNull expects objects).

** Terminals: **
- `assertEquals`
- `assertTrue`
- `assertFalse`
- `assertNull`
- `assertNotNull`
- `0`
- `1`
- `5`
- Variable name from the prefix

** Grammar Rules: **

1. `A -> assertEquals(const | var)`
2. `A -> assertTrue(var)`
3. `A -> assertFalse(var)`
4. `A -> assertNull(var)`
5. `A -> assertNotNull(var)`

6. `const -> 0 | 1 | 5`
7. `var -> type matching var from the prefix`

Hint 1: there were 5 possible assertions for the first test prefix. 
Hint 2: not every developer written assertion fits this grammar! 

You might have to tinker with your prompt again. Once your satisifed, record your results and proceed to the next section.

**Task: Evaluation**

Based on the outputs you recorded, evaluate each approach as follows:
1. **Correctness** - 
    a. How many assertions exactly match the developer written assertion?
    b. How many assertions were syntactically different, but semantically equivalent?
    c. How many assertions successfully compile?

2. **Error Analysis** - In the cases that the model was not able to generate the correct assertion, describe the error. For the grammar based approach, is there a correct assertion that fits the given grammar?

3. **Comparative Analysis** - Overall, which approach was more effective and why?

## 2. IR Based Approach

**Task: Implement Jaccard** 

In this part you will implement an information retreival approach for assertion generation.

Given a test prefix for which we want to generate an assertion and a corpus of tests, do the following:
1. Break the test into a set of words
2. For each document in the corpus, break it into a set of words and compute the jaccard similarity.
3. "Retrieve" the assertion for the most similar test

You can download the starter code: TODO XXXXXXX

You'll need a corpus to retrieve assertions from.

TODO: find a corpus and add a download link.

**Evaluation**
Perform the same evaluation as in Part 1. For the error analysis, describe how similar the retrieved assertion is to the correct assertion. Include a comparative analysis between all three approaches.

## Submission 

Submit the following files to Gradescope:
1. Evaluation document answering the questions for all three approaches.
2. Prompts for both generation and ranking
3. Jaccard code
