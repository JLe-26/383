---
layout: default
title: Test Oracle Genearation
type: Homework
number: 02
active_tab: homework
release_date: 2025-02-19
due_date: 2025-02-24 23:59:00EDT

---

<!-- Check whether the assignment is ready to release -->
{% capture today %}{{'now' | date: '%s'}}{% endcapture %}
{% capture release_date %}{{page.release_date | date: '%s'}}{% endcapture %}
{% if release_date > today %} 
<div class="alert alert-danger">
Warning: this assignment is out of date.  It may still need to be updated for this year's class.  Check with your instructor before you start working on this assignment.
</div>
{% endif %}
<!-- End of check whether the assignment is up to date -->


<!-- Check whether the assignment is up to date -->
{% capture this_year %}{{'now' | date: '%Y'}}{% endcapture %}
{% capture due_year %}{{page.due_date | date: '%Y'}}{% endcapture %}
{% if this_year != due_year %} 
<div class="alert alert-danger">
Warning: this assignment is out of date.  It may still need to be updated for this year's class.  Check with your instructor before you start working on this assignment.
</div>
{% endif %}
<!-- End of check whether the assignment is up to date -->



{% if page.materials %}
<div class="alert alert-info">
You can download the materials for this assignment here:
<ul>
{% for item in page.materials %}
<li><a href="{{item.url}}">{{ item.name }}</a></li>
{% endfor %}
</ul>


<i>Remember to make a copy of the notebook into your own Drive by choosing “Save a Copy in Drive” from Colab’s “File” menu.</i>

</div>
{% endif %}





{{page.type}} {{page.number}}: {{page.title}}
=============================================================

_Due: {{page.due_date}}_

You can optionally work with a partner on this assignment. You clearly state in your sumbission if you are working with a partner and their name.

Given an input for a system, the challenge of distinguishing the corresponding desired, correct behaviour from potentially incorrect behavior is called the *test oracle problem*. 

## Overview

In this assignment, you will implement and evaluate two algorithms for test oracle generation:

- Neural Approach 
- IR Approach

### Setup: 

1. Get a free API key from [Hugging Face](https://huggingface.co/)  
      a. Create an account  
      b. Create a new [access token](https://huggingface.co/settings/tokens). The only permission needed is **"Make calls to inference providers"**. Save the token somewhere safe.  

2. Install dependencies:  TODO remove this i dont think they need it.
    a. `wget https://repo1.maven.org/maven2/org/json/json/20250107/json-20250107.jar`

3. Agree to the terms: <link to the hugging face agreement here>

4. Download the benchmark:  
    a. `wget .... hw2/....` I need to finish making the files.

## 1. Neural Approach

Given a test prefix and a method under test, use a language model to generate the assertion.

**Program Representation** 
Language model based approaches consider the program as a sequence of words. 

### A Background on Large Language Models

The core task for most state-of-the-art LLMs is word prediction. Given a sequence of words, what is the probability distribution of the next word?

For example, given the sequence "Listen to your -" the most likely next words might be: heart, gut, body, parents, grandma, etc. This might look like the probability distribution shown below.

[Next Word Probability](https://towardsdatascience.com/wp-content/uploads/2023/07/1_yFX4vrio7Io2tnVWbW8xQ.png)

**Training**
For this course, we won't go into the specifics of training an LLM. You can think of the training stage as creating a *black-box* which calculates this probability distribution by looking at a large corpus of text.  After training, we have an artifact which, given a sequence of text, can output the next most probable token.

The training portion is highly expensive, costing millions of dollars and taking months to complete. Even with state of the art hardware! Luckily, LLMs like ChatGPT, Gemini, and DeepSeek are "pretrained" andwe can query the model without needing to do any training ourselves.

**Inference**
Inference refers to *querying* the model. This is much less expensive than training and does not modify the underlying model. This is what happens when you ask chatGPT a question in the web interface.

### Querying an LLM to Generate Assertions

I've included [starter code](https://github.com/bmc-cs-software-analysis/383/tree/main/hws/hw02/queryModel.sh) to query an LLM. For this assignment we'll use Mistral-7b: a model trained on both code and natural language. The "7b" refers to the size of the model (7 billion parameters). This is quite small in terms of modern LLMs. Chat-GPT4 has approximately 1.8 *trillion* parameters. 

To begin, open the starter code and save your `API_KEY` in the variable on line 1. Run `bash queryModel.sh` to see the response to the query (prompt) on line 2: `"Where is the best airport in the United States?"`

The output is returned as a json string with the output text in `choices[0].message.content`.

Try it out on a few other input prompts of your choice.

**Task: Generating an Assertion**

Now let's use this model to generate assertions given a test prefix. 

TODO: I NEED A BENCHMARK. Methods2Test dataset? Maybe 10 from here?

**Task: Ranking Assertions**

Given the following grammar, exhaustively enumerate all possible assertions and ask the LLM to rank them. 

TODO: Give the grammar

**Task: Evaluation**

Based on the outputs you recorded, evaluate each approach as follows:
1. **Correctness** - 
    a. How many assertions exactly match the developer written assertion?
    b. How many assertions were syntactically different, but semantically equivalent?
    c. How many assertions successfully compile?

2. **Error Analysis** - In the cases that the model was not able to generate the correct assertion, describe the error. For the grammar based approach, is there a correct assertion that fits the given grammar?

3. **Comparative Analysis** - Overall, which approach was more effective and why?

## 2. IR Based Approach

**Task: Implement Jaccard** 

In this part you will implement an information retreival approach for assertion generation.

Given a test prefix for which we want to generate an assertion and a corpus of tests, do the following:
1. Break the test into a set of words
2. For each document in the corpus, break it into a set of words and compute the jaccard similarity.
3. "Retrieve" the assertion for the most similar test

You can download the starter code: TODO XXXXXXX

You'll need a corpus to retrieve assertions from.

TODO: find a corpus and add a download link.

**Evaluation**
Perform the same evaluation as in Part 1. For the error analysis, describe how similar the retrieved assertion is to the correct assertion. Include a comparative analysis between all three approaches.

## Submission 

Submit the following files to Gradescope:
1. Evaluation document answering the questions for all three approaches.
2. Prompts for both generation and ranking
3. Jaccard code
